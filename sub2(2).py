# -*- coding: utf-8 -*-
"""sub2(2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sR8fbSFZF7zjSQfQzyWCMUFmGTlQVld9

# *Projek Rekomendasi*

---



**Nama : Ardiansyyah Putra**

**dataset diambil dari** : https://www.kaggle.com/datasets/saurabhbagchi/books-dataset

___
# Importing Libraries and Dataset

## Importing Libraries

Pada notebook ini saya melakukan import pada beberapa libraries, seperti pandas, numpy, tenserflow, dll.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

"""load dataset dengan API Kaggle"""

import os

os.environ['KAGGLE_USERNAME'] = 'hyotoo'
os.environ['KAGGLE_KEY'] = '4fa09ef5ec8c7973cf7d763bafbada38'

"""# **Load Dataset**"""

!kaggle datasets download -d saurabhbagchi/books-dataset

"""unzip dataset hasil download"""

!unzip books-dataset.zip

"""## Importing Dataset

Pada notebook ini, hanya akan menggunakan dataframe book dan rating untuk content based dan collaborative filtering recommendation
"""

df1 = pd.read_csv("/content/books_data/books.csv", encoding='ISO-8859-1', sep=';', on_bad_lines='skip')
df2 = pd.read_csv("/content/books_data/ratings.csv", encoding='ISO-8859-1', sep=';', on_bad_lines='skip')

"""# **Data Understanding**"""

df1

df1.info()

"""data dari dataframe books memiliki 271360 baris dan 8 koolom data

Meneliti ukuran dari dataframe rating
"""

df2

df2.info()

df2.describe()

"""data dari dataframe rating memiliki 11449780 baris dan 3 koolom data

rating 0 dalam data sangat banyak, dan saya memilih untuk mennghapusnya

# Data Preprocessing

Memberi nama header baru pada kolom Book-Rating dan User-ID pada df2
"""

df2 = df2.rename(columns={'Book-Rating': 'rating','User-ID':'user_id'})

"""Melihat dari dataframe dari rating dan books terbilang banyak, di sini hanya diambil 30000 row dari book dataset dan 20000 row untuk rating dataset jadi masih terllihat cukup banyak"""

df1 = df1[:30000]
df2 = df2[:20000]

df2 = df2.drop(df2[df2.rating == 0].index)

"""melakukan drop atau pembuangan pada row - row yang merupakan duplikasi dari row - row yang lain, sehingga dataset kita tetap memiliki integritas dan tidak berulang"""

df1 = df1.dropna()
df2 = df2.dropna()

df2 = df2.drop_duplicates()
df1 = df1.drop_duplicates()

df1.shape

df2.shape

"""setelah pengahpusan rating 0 data menjadi menjadi 7340

5 data teratas dari dataset rating
"""

df2.head()

"""5 data teratas dari dataset buku"""

df1.head()

"""Memberi nama header baru pada kolom Book-Title, Book-Author, Image-URL-S, Image-URL-S,Image-URL-M,Image-URL-L pada book_dataset menjadi huruf kecil"""

df1 = df1.rename(columns={'Book-Title': 'book_title','Book-Author':'book_author','Year-Of-Publication':'year_of_publication','Image-URL-S':'Image_URL_S','Image-URL-M':'Image_URL_M','Image-URL-L':'Image_URL_L'})

"""## Univariate Data Analys

Meneliti distribusi rating dari rating dataframe dengan Barplot
"""

count = df2["rating"].value_counts()
count.plot(kind='bar', title="Rating");

plt.show()

"""rata rata user memberikan rating 8.0

Meneliti distribusi tahun terbitnya buku dari book dataframe dengan Barplot
"""

count = df1["year_of_publication"].value_counts()
count.plot(kind='bar', title="Year of Publication");

plt.show()

"""dari data tersebut dapat diketahui banyak buku yang terbit pada tahun 2002

## Multivariate Analysis

Melihat pairplot yang ada pada rating dataset
"""

import seaborn as sns
sns.pairplot(df2, diag_kind = 'kde')

"""___
# Content Based Filtering

Content-Based Filtering adalah metode sistem rekomendasi yang memberikan rekomendasi kepada pengguna berdasarkan karakteristik atau atribut dari item (konten) yang disukai atau pernah dinikmati oleh pengguna tersebut sebelumnya.

## Data Preparation

perlu mengubah dataframe dari buku menjadi sebuah list
"""

book_ISBN = df1['ISBN'].tolist()

book_title = df1['book_title'].tolist()

book_author = df1['book_author'].tolist()

book_year_of_publication = df1['year_of_publication'].tolist()

"""Setelah kita membuat list, kita perlu membuat dictionary yang digunakan untuk memnentukan pasangan key-value pada book_ISBN, book_title, book_author, dan book_year_of_publication."""

book = pd.DataFrame({
    'book_ISBN': book_ISBN,
    'book_title': book_title,
    'book_author': book_author,
    'book_year_of_publication': book_year_of_publication
})
book

"""## Modeling

Pada content Based Filtering, kita akan menggunakan TF-IDF Vectorizer untuk membangun sistem rekomendasi berdasarkan penulis buku.

TF-IDF yang merupakan kepanjangan dari Term Frequency-Inverse Document Frequency memiliki fungsi untuk mengukur seberapa pentingnya suatu kata terhadap kata - kata lain dalam dokumen.
Kita umumnya menghitung skor untuk setiap kata untuk menandakan pentingnya dalam dokumen dan corpus.

Pada cell code di bawah ini kita akan mengambil kata - kata penting dalam kolom book_author
"""

from sklearn.feature_extraction.text import TfidfVectorizer

tf = TfidfVectorizer()

tf.fit(book['book_author'])

tf.get_feature_names_out()

"""Kemudian kita akan lakukan fit dan transformasi ke dalam matriks, pada code di bawah ini, matriks tersebut adalah tfidf_matrix"""

tfidf_matrix = tf.fit_transform(book['book_author'])

tfidf_matrix.shape

"""Pada tfidf_matrix terdapat 30000 ukuran data dan 11444 nama penulis buku

.todense(), atribut ini dipakai untuk mengubah tfidf_matrix yang awalnya vektor menjadi matriks
"""

tfidf_matrix.todense()

"""Dataframe di bawah ini digunakan untuk melihat matriks dari judul buku dengan penulis - penulis buku"""

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=book.book_title
).sample(10, axis=1,replace=True).sample(20, axis=0)

"""Dalam sistem rekomendasi, kita perlu mencari cara supaya item yang kita rekomendasikan tidak terlalu jauh dari data pusat, oleh karena itu kita butuh derajat kesamaan pada item, dalam proyek ini, buku dengan derajat kesamaan antar buku dengan cosine similarity"""

from sklearn.metrics.pairwise import cosine_similarity

cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

"""Pada code di bawah ini kita akan membuat dataframe cosine_sim_df dengan baris dan kolomnya adalah judul dari buku"""

cosine_sim_df = pd.DataFrame(cosine_sim, index=book['book_title'], columns=book['book_title'])
print('Shape:', cosine_sim_df.shape)

cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""Di bawah ini adalah fungsi untuk mendapatkan rekomendasi berbasis penulis buku dengan k sebagai jumlah rekomendasi yang diingkan, dalam fungsi ini, kita akan mendapatkan 3 rekomendasi

Atribut argpartition berguna untuk mengambil sejumlah nilai k, dalam fungsi ini 5 tertinggi dari tingkat kesamaan yang berasal dari dataframe cosine_sim_df.
"""

def author_recommendations(i, M, items, k=10):
    ix = M.loc[:,i].to_numpy().argpartition(range(-1,-k,-1))
    closest = M.columns[ix[-1:-(k+2):-1]]
    closest = closest.drop(i, errors='ignore')
    return pd.DataFrame(closest).merge(items).head(k)

"""Pada code cell di bawah ini, kita akan mencari rekomendasi dari buku yang sudah dibaca, dalam kasus ini, buku yang sudah dibaca adalah "Spider-Man" yang ditulis oleh Peter David dan terbit pada tahun 2002"""

books_that_have_been_read = "Spider-Man"
book[book.book_title.eq(books_that_have_been_read)]

"""Pada cell code di bawah ini, kita akan mendapatkan 5 rekomendasi dari buku "The Diaries of Adam and Eve"
"""

recommendations = author_recommendations(books_that_have_been_read, cosine_sim_df, book[['book_title', 'book_author']])

"""Pada beberapa kasus, rekomendasi akan memberikan rekomendasi yang terduplikat, sehingga perlu dibuang rekomendasi yang terduplikat"""

recommendations = recommendations.drop_duplicates()

"""Berikut adalah 10 buku rekomendasi yang ditulis oleh Peter David"""

recommendations

"""## Evaluation

Kita akan memakai metrik evaluasi Precision@k di mana:

Jumlah rekomendasi yang relevan / jumlah total buku relevan dalam dataset

Variabel books_that_have_been_read_row di bawah ini akan mengambil satu row dari buku yang pernah dibaca sebelumnya, dan variabel books_that_have_been_read_author adalah penulis buku dari buku yang pernah dibaca sebelumnya
"""

books_that_have_been_read_row = df1[df1.book_title == books_that_have_been_read]
books_that_have_been_read_author = books_that_have_been_read_row.iloc[0]["book_author"]

"""relevant_recomendations menunjukan buku yang relevan dari rekomendasi yang diberikan"""

# Ambil rekomendasi yang relevan (dari author yang sama)
relevant_recommendations = recommendations[recommendations.book_author == books_that_have_been_read_author]

# Hitung Precision@k
k = recommendations.shape[0]
precision_at_k = (relevant_recommendations.shape[0] / k) * 100
print(f"Precision@{k}: {precision_at_k:.2f}%")

"""hal ini karena dari top 10 rekomendasi ada satu buku yang bukan Peter David authornya

___
# Collaborative Filtering

## Data Preparation

Pada cell code di bawah ini, saya akan meyandikan user_id menjadi integer
"""

user_ids = df2['user_id'].unique().tolist()

user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}

user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

"""Pada cell code di bawah ini, saya akan meyandikan book_id menjadi integer"""

book_ids = df2['ISBN'].unique().tolist()
book_to_book_encoded = {x: i for i, x in enumerate(book_ids)}
book_encoded_to_book = {i: x for i, x in enumerate(book_ids)}

df2['user'] = df2['user_id'].map(user_to_user_encoded)
df2['book'] = df2['ISBN'].map(book_to_book_encoded)

"""Terakhir, kita akan cek jumlah pembaca dan jumlah buku, serta mengubah tipe data rating menjadi float"""

num_users = len(user_encoded_to_user)
print("User", + num_users)
num_book = len(book_encoded_to_book)
print("Book",num_book)
df2['rating'] = df2['rating'].values.astype(np.float32)

min_rating = min(df2['rating'])
max_rating = max(df2['rating'])

print('Number of User: {}, Number of Book: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_book, min_rating, max_rating
))

"""## Membagi Dataset

Sebelum kita membagi dataset menjadi data latih dan data validasi, kita terlebih dahulu harus mengacak dataset
"""

df2 = df2.sample(frac=1, random_state=42)
df2

"""Pada cell code di bawah ini, saya membagi dataset yang ada menjadi 70% untuk latihan dan 30% untuk validasi"""

x = df2[['user', 'book']].values

y = df2['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

train_indices = int(0.70 * df2.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""## Model Development

Model yang akan kita pakai dalam sistem rekomendasi berbasis pendapat pengguna adalah RecommenderNet
"""

from tensorflow import keras
from tensorflow.keras import layers
import tensorflow as tf

class RecommenderNet(tf.keras.Model):

  def __init__(self, num_users, num_resto, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_resto = num_resto
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding(
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1)
    self.resto_embedding = layers.Embedding(
        num_resto,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.resto_bias = layers.Embedding(num_resto, 1)

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0])
    user_bias = self.user_bias(inputs[:, 0])
    resto_vector = self.resto_embedding(inputs[:, 1])
    resto_bias = self.resto_bias(inputs[:, 1])

    dot_user_resto = tf.tensordot(user_vector, resto_vector, 2)

    x = dot_user_resto + user_bias + resto_bias

    return tf.nn.sigmoid(x)

"""Selanjutnya kita melakukan proses compile pada model dengan binary crossentropy sebagai loss function, adam sebagai optimizer, dan RMSE sebagai metrik dari model"""

model = RecommenderNet(num_users, num_book, 50)

model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""Selanjutnya kita akan melatih model dengan batch_size 5 dan 20 epochs"""

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 3,
    epochs = 25,
    validation_data = (x_val, y_val)
)

"""Evaluasi dilakukan menggunakan metrik Root Mean Squared Error (RMSE) untuk mengukur sejauh mana prediksi rating yang dihasilkan oleh model mendekati nilai rating sebenarnya.

Berdasarkan hasil pelatihan selama 25 epoch, diperoleh:

RMSE pada data pelatihan sebesar 0.0332

RMSE pada data validasi sebesar 0.2208

Nilai RMSE yang rendah menunjukkan bahwa model mampu memprediksi rating dengan akurasi yang cukup baik. Semakin kecil nilai RMSE, semakin dekat hasil prediksi terhadap rating sebenarnya. Dalam konteks ini, RMSE validasi sebesar 0.2208 dapat dikategorikan sebagai performa yang cukup akurat

## Visualisasi Metrik
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""## Mendapatkan Rekomendasi"""

df1 = df1
df2 = df2

"""Pada cell code di bawah ini, kita akan mengambil user_id secara acak dari rating_dataset. Dari user_id ini kita perlu mengetahui buku - buku apa saja yang pernah dibaca dan yang belum pernah dibaca, sehingga kita hanya dapat merekomendasikan buku - buku yang belum dibaca."""

user_id = df2.user_id.sample(1).iloc[0]
books_have_been_read_by_user = df2[df2.user_id == user_id]

books_have_not_been_read_by_user = df1[df1['book_ISBN'].isin(books_have_been_read_by_user.ISBN.values)]['book_ISBN']
books_have_not_been_read_by_user = list(
    set(books_have_not_been_read_by_user)
    .intersection(set(book_to_book_encoded.keys()))
)

books_have_not_been_read_by_user = [[book_to_book_encoded.get(x)] for x in books_have_not_been_read_by_user]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(books_have_not_been_read_by_user), books_have_not_been_read_by_user)
)

"""Pada code cell terakhir di bawah ini, kita akan merekomendasikan 10 buku dari user"""

ratings = model.predict(user_book_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_book_ids = [
    book_encoded_to_book.get(books_have_not_been_read_by_user[x][0]) for x in top_ratings_indices
]

top_books_recommended = (
    books_have_been_read_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .ISBN.values
)

books_row = df1[df1['book_ISBN'].isin(top_books_recommended)]
for row in books_row.itertuples():
    print(row.book_title, ':', row.book_author)

print('----' * 8)
print('Top 10 Book Recommendation for user: {}'.format(user_id))
print('----' * 8)

recommended_books = df1[df1['book_ISBN'].isin(recommended_book_ids)]
for row in recommended_books.itertuples():
    print(row.book_title, ':', row.book_author)

# Ambil semua buku yang pernah diberi rating oleh user
user_actual_ratings = df2[df2['user_id'] == user_id]

# Buku yang dianggap relevan = rating >= 4
relevant_books = user_actual_ratings[user_actual_ratings['rating'] >= 8]['ISBN'].tolist()

# Precision@10: Berapa dari rekomendasi yang memang relevan
relevant_recommendations = [book for book in recommended_book_ids if book in relevant_books]
precision_at_10 = len(relevant_recommendations) / 10

print(f'\nPrecision@10 untuk user {user_id}: {precision_at_10 * 100:.2f}% ({len(relevant_recommendations)} dari 10 rekomendasi relevan)')